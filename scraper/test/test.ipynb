{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import random\n",
    "from tweet_class import Tweet\n",
    "import re\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import uuid\n",
    "import logging\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url = \"https://x.com/search?f=live&q=%28from%3ARER_A%29+%28to%3ARER_A%29+until%3A2024-12-03&src=typed_query\"\n",
    "url = 'https://x.com/RER_A/status/1802654385207480781'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir=\"/home/seluser/twitter_scraper/selenium\"\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--mute-audio\")\n",
    "options.add_experimental_option(\"detach\", True)\n",
    "options.add_argument('--ignore-certificate-errors')\n",
    "options.add_argument('--ignore-ssl-errors')\n",
    "options.add_argument(f\"--user-data-dir={data_dir}\")\n",
    "options.add_argument(\"--disable-blink-features=AutomationControlled\") \n",
    "options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"]) \n",
    "options.add_experimental_option(\"useAutomationExtension\", False) \n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "chrome.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "already_done_set = set()\n",
    "already_done_set_with_threadid = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _scrap_thread(chrome, thread_id):\n",
    "        \"\"\"\n",
    "        Scrap all the tweet in a thread belonging to the account mentioned in the constructor\n",
    "        A thread is a tweet that has been replied to by other tweet\n",
    "\n",
    "        To check if a tweet in the last from the thread it is either the last tweet of the thread or is followed by a separator\n",
    "        Args:\n",
    "            chrome (webdriver): The webdriver to use\n",
    "            thread_id (str): The ID of the thread to link all the tweet in the thread\n",
    "        \n",
    "        Returns:\n",
    "            None: If everything works, nothing is returned otherwise an error is raised\n",
    "        \"\"\"\n",
    "        subtweets = chrome.find_elements('xpath', '//article[@data-testid=\"tweet\"]')\n",
    "        for index, subtweet in enumerate(subtweets):\n",
    "                print(subtweet.text)\n",
    "                #last_thread_tweet = subtweet.find_elements('xpath', '../../../div/div/div//text()')\n",
    "                try:\n",
    "                    last_thread_tweet = subtweet.find_elements('xpath', './parent::div/parent::div/parent::div/following-sibling::div')[0]\n",
    "\n",
    "\n",
    "                    elements = subtweet.find_elements('xpath', './div/div/div')\n",
    "                    content = elements[-1].find_elements('xpath', './div[2]/div')\n",
    "                    header = content[0]\n",
    "                except:\n",
    "                    print(\"continue\")\n",
    "                    continue\n",
    "                \n",
    "                # Check if the tweet is from the account mentioned in the constructor\n",
    "                if 'RER_A' not in header.text:\n",
    "                    return thread_id\n",
    "\n",
    "                try:\n",
    "                    if index==0:\n",
    "                        time_posted = scrap_tweet(subtweet, 'Normal')\n",
    "                    else:\n",
    "                        time_posted = scrap_tweet(subtweet, 'Réponse')\n",
    "                except Exception as e:\n",
    "                    logging.error(\"Error while scrapping tweet in thread\")\n",
    "                    logging.error(f\"Thread link :{chrome.current_url}, reponse index : {index}\")   \n",
    "                    continue\n",
    "                \n",
    "\n",
    "                if time_posted in already_done_set:\n",
    "                    thread_id = already_done_set_with_threadid.get(time_posted)\n",
    "                already_done_set.add(time_posted)\n",
    "                already_done_set_with_threadid[time_posted] = thread_id\n",
    " \n",
    "                #if last_thread_tweet == \"\" or index == len(subtweets) - 1:\n",
    "                #    return\n",
    "                if last_thread_tweet.text == \"\" or index == len(subtweets) - 1:\n",
    "                    return thread_id\n",
    "        return thread_id\n",
    "\n",
    "def scrap_tweet(tweet, type):\n",
    "    \"\"\"\n",
    "    Scrap a tweet and return a Tweet object with all the information retrieved\n",
    "    There a special cae when the tweet is a quote tweet, in this case, the tweet content correspond to the date of the quoted tweet\n",
    "    Args:\n",
    "        tweet (WebElement): The tweet to scrap\n",
    "        type (str): The type of tweet (Normal or Réponse)\n",
    "    Returns:\n",
    "        Tweet: The tweet object with all the information retrieved\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        elements = tweet.find_elements('xpath', './div/div/div[2]')\n",
    "        content = elements[-1].find_elements('xpath', './div[2]/div')\n",
    "    except Exception as e:\n",
    "        raise \"Couldn't get tweet content\"\n",
    "    \n",
    "    header = content[0]\n",
    "    time_posted = header.find_element('xpath', './/time').get_attribute('datetime')\n",
    "    time_posted_conv = datetime.strptime(time_posted, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    # Check if the tweet is a quote tweet\n",
    "    try:\n",
    "        date_cited_tweet = content[-2].find_elements('xpath','.//time')[0].get_attribute('datetime')\n",
    "        time_posted_conv_2 = datetime.strptime(date_cited_tweet, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "        print('citation')\n",
    "        return time_posted_conv\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        content_tweet = content[-2]\n",
    "        #print(content_tweet.text)\n",
    "\n",
    "        elements = content_tweet.find_elements(By.XPATH, \"./div//*\")\n",
    "        content_list = []\n",
    "\n",
    "        for element in elements:\n",
    "            if element.tag_name == 'span':\n",
    "                content_list.append(element.text)\n",
    "            elif element.tag_name == 'img':\n",
    "                content_list.append(element.get_attribute('alt'))\n",
    "        #print(content_list)\n",
    "    except:\n",
    "        raise(\"Aie\")\n",
    "\n",
    "\n",
    "    return time_posted_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RER A\n",
      "@RER_A\n",
      "Suivre\n",
      "Le train actuellement à Châtelet–Les Halles et en direction de Saint-Germain-en-Laye stationne en raison d'une gêne à la fermeture des portes. #RERA\n",
      "12:47 PM · 17 juin 2024\n",
      "·\n",
      "13,4 k\n",
      " vues\n",
      "2\n",
      "1\n",
      "6\n",
      "continue\n",
      "RER A\n",
      "@RER_A\n",
      "·\n",
      "17 juin\n",
      " Le trafic est très perturbé de Marne-la-Vallée–Chessy • Boissy-Saint-Léger vers Cergy-Le Haut • Poissy et Saint-Germain-en-Laye en raison d'un train en panne à Châtelet–Les Halles. #RERA\n",
      "2\n",
      "1\n",
      "7\n",
      "4 k\n",
      "RER A\n",
      "@RER_A\n",
      "·\n",
      "17 juin\n",
      " Le trafic est perturbé de Marne-la-Vallée–Chessy • Boissy-Saint-Léger vers Cergy-Le Haut • Poissy et Saint-Germain-en-Laye en raison d'un train en panne à Châtelet–Les Halles. #RERA\n",
      "1\n",
      "2\n",
      "5 k\n",
      "RER A\n",
      "@RER_A\n",
      "·\n",
      "17 juin\n",
      " Le trafic est rétabli  de Marne-la-Vallée–Chessy • Boissy-Saint-Léger vers Cergy-Le Haut • Poissy et Saint-Germain-en-Laye (train en panne). #RERA\n",
      "1\n",
      "3 k\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'000000'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_scrap_thread(chrome, \"000000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error while scrapping main tweet\n"
     ]
    }
   ],
   "source": [
    "tweets_on_page = chrome.find_elements('xpath', '//article[@data-testid=\"tweet\"]')\n",
    "main_page = chrome.window_handles[0]\n",
    "for tweet in tweets_on_page:\n",
    "\n",
    "\n",
    "    thread_id = str(uuid.uuid4())\n",
    "\n",
    "    # Scrap the main tweet informations\n",
    "    try:\n",
    "        time_posted_conv = scrap_tweet(tweet, 'Réponse')\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error while scrapping main tweet\")\n",
    "        break\n",
    "    \n",
    "    \n",
    "    print(\"-------------------\")\n",
    "    print(time_posted_conv)\n",
    "    print(already_done_set)\n",
    "    print(time_posted_conv in already_done_set)\n",
    "    print(already_done_set_with_threadid)\n",
    "    print(\"-------------------\")\n",
    "    \n",
    "    \n",
    "    # Check if the tweet has already been scrapped\n",
    "    if time_posted_conv in already_done_set:\n",
    "        continue\n",
    "    already_done_set.add(time_posted_conv)\n",
    "\n",
    "\n",
    "\n",
    "    # Click sur le tweet si adresse est sur X sinon rien\n",
    "    # Click on the tweet to access the thread\n",
    "    try:\n",
    "        #tweetContent = tweet.find_element('xpath', './/div[@data-testid=\"tweetText\"]')\n",
    "        tweetThreadLink = tweet.find_element(\"xpath\", './/a[contains(@href, \"/status/\")]')\n",
    "        ActionChains(chrome).key_down(Keys.CONTROL).click(tweetThreadLink).key_up(Keys.CONTROL).perform()\n",
    "        chrome.switch_to.window(chrome.window_handles[-1])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"No CLICK\")\n",
    "        continue\n",
    "\n",
    "    time.sleep(5 + random.randint(5, 10))\n",
    "\n",
    "    # Scrap the thread\n",
    "    try:\n",
    "        res = _scrap_thread(chrome, thread_id)\n",
    "        if res != 0:\n",
    "            thread_id = res\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error while scrapping thread\")\n",
    "        chrome.close()  \n",
    "        chrome.switch_to.window(main_page) \n",
    "        raise e\n",
    "    \n",
    "\n",
    "    already_done_set_with_threadid[time_posted_conv] = thread_id\n",
    "\n",
    "\n",
    "\n",
    "    #time.sleep(delay_before_closing_tab + random.randint(4, 7))\n",
    "        \n",
    "    # Close the new tab\n",
    "    chrome.close()\n",
    "\n",
    "    # Switch back to the original tab\n",
    "    chrome.switch_to.window(main_page) \n",
    "    WebDriverWait(chrome, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//article[@data-testid='tweet']\"))\n",
    "    )\n",
    "    time.sleep(5 + random.randint(5, 10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
